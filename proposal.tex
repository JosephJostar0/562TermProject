\documentclass[12pt]{article}

% --- Basic Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times} % Use Times font
\usepackage[margin=1in]{geometry} % 1-inch margins
\usepackage{graphicx}
\usepackage{enumitem} % For custom lists
\usepackage{parskip} % For better paragraph spacing

% --- Links and References ---
% hyperref must be placed later
\usepackage{url} % Handle URLs
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref}

% --- Title Information (Optimized) ---
\title{
    \textbf{TCSS 562 Term Project Proposal} \\
    \large A Case Study on the Performance, Cost, and Architectural Implications of LLM-Generated Serverless Image Processing Pipelines
}

% Author, Course, and Contact info merged into \author block
\author{
    % Waiting for the 4th member \\
    Bohan Xiong \textbf{\texttt{(bohanx2@uw.edu)}} \\
    Xu Zhu \texttt{(xuz44@uw.edu)}\\
    Xiaoling Wei \texttt{(xlwei28@uw.edu)}\\
    % \vspace{1em} % Add vertical spacing
    % \normalsize
    % \textbf{Instructor:} Wes J. Lloyd \\
    % \textbf{Contact Email:} \texttt{email@uw.edu}
}

\date{\textbf{Instructor:} Wes J. Lloyd}
% \date{October 22, 2025}

% --- Document Start ---
\begin{document}

\maketitle

% --- Application Description ---
\section{Cloud Application Description}
Our team will implement an enhanced version of the predefined Image Processing Pipeline. This application will be built as an event-driven system on the AWS Lambda FaaS platform, which is known for its lightweight virtualization technology like Firecracker \cite{firecracker}.

To conduct a deeper analysis of LLM code generation capabilities, our pipeline will consist of a sequential chain of five serverless functions. We have intentionally designed this pipeline to include functions with different computational loads (CPU-intensive vs. I/O-intensive) to evaluate LLM performance across varied workloads.

The pipeline will trigger on an S3 upload and execute the following logical steps:
\begin{enumerate}
    \item \textbf{Function 1 (Greyscale):} Converts the input image to greyscale.
    \item \textbf{Function 2 (Resize):} Resizes the image to a fixed resolution (e.g., 800x600).
    \item \textbf{Function 3 (Color Depth Map):} Performs a CPU-intensive operation, such as mapping a 10-bit color depth image to 8-bit.
    \item \textbf{Function 4 (Rotate):} Rotates the resized image by 90 degrees.
    \item \textbf{Function 5 (Format Convert):} Performs an I/O-intensive operation, writing the final image as a different format (e.g., from JPEG to PNG) in the destination S3 bucket.
\end{enumerate}

% --- Design-Tradeoffs ---
\section{Design-Tradeoffs to be Investigated}
Our case study will be two-dimensional, focusing on one primary and one secondary design tradeoff.

\subsection{Primary Investigation: LLM Comparison Theme}
Our main goal is to compare the quality and efficiency of code generated by state-of-the-art Large Language Models. We will provide identical, detailed prompts for each of the five functions to three distinct LLMs:
\begin{itemize}
    \item GPT-5 Thinking\footnote{Should the LLM providers upgrade or change their models during the experiment (e.g., release a new version), we will use the corresponding upgraded model available at that time to ensure the relevancy of our study.}
    \item Gemini 2.5 Pro
    \item DeepSeek V2.5 Research
\end{itemize}
We will instruct all LLMs to generate the function code in \textbf{Python 3.13}\footnote{If a newer Python environment is provided by the serverless platform at the time of the experiment, we will switch to the latest, such as the newly released Python 3.14.0.}. We chose this version as it is modern, well-supported by current AWS Lambda runtimes, and provides a relevant baseline for performance evaluation.

This will result in three functionally identical but implementation-distinct application versions. By including both CPU and I/O-intensive tasks, we can deeply analyze the differences in code optimization (e.g., algorithmic efficiency, I/O handling) among the LLMs.

\subsection{Secondary Investigation: CPU Architecture}
As an extension, we will investigate the impact of the underlying CPU architecture. We will deploy each LLM-generated pipeline version onto both available AWS Lambda architectures:
\begin{itemize}
    \item Intel (x86\_64)
    \item ARM (Graviton2)
\end{itemize}
This allows us to explore potential performance and cost differences (e.g., ARM's \textasciitilde20\% price discount) and observe if certain LLMs produce code that is better optimized for a specific architecture. Our investigation will build upon existing research in this area, such as the work by Chen et al. \cite{chen2023}, which also compared these two architectures on AWS Lambda.

% --- Proposed Evaluation Metrics ---
\section{Proposed Evaluation Metrics}
Our evaluation will compare all pipeline versions across performance and cost dimensions. We will use an identical set of input images and run repeated tests (e.g., 100+ runs) to gather statistical data.
\begin{itemize}
    \item \textbf{Per-Function Runtime Analysis (Performance):} The average execution time (in milliseconds) for each individual function. This metric will be key to comparing how each LLM's code performs on CPU-intensive (Color Depth Map) vs. I/O-intensive (Format Convert) tasks.
    \item \textbf{Average Pipeline Turnaround Time (Performance):} The average end-to-end turnaround time (in seconds) for the complete five-function pipeline.
    \item \textbf{Hosting Cost (Cost):} The estimated total hosting cost (in \$) for processing a large, hypothetical workload of 100,000 images for each LLM-generated pipeline version.
    \item \textbf{Cross-Architecture Performance (Perf/Cost):} A direct comparison of the runtime and cost for the same code (e.g., Gemini's) running on x86 vs. ARM.
\end{itemize}
For all performance metrics, we will report the statistical average, standard deviation, and coefficient of variation (CV).

% --- Work Plan ---
\section{Work Plan}
Our team will use Git/GitHub for version control and collaborate remotely via Discord.
\begin{itemize}
    \item \textbf{Phase 1 (Oct 22 - Oct 31): Feasibility Spike:} Manually create and test an AWS Lambda Layer containing the \texttt{Pillow} library. Deploy one test function (e.g., Greyscale) to confirm the dependency is correctly loaded. Following this validation, finalize prompts and generate all function code (5 functions x 3 LLMs) from all LLMs.
    \item \textbf{Phase 2 (Nov 1 - Nov 8):} Deploy all 6 pipeline versions (3 LLMs x 2 Architectures) on AWS Lambda.
    \item \textbf{Phase 3 (Nov 9 - Nov 16):} Develop test harness, potential adapting or using concepts from the SAAF framework mentioned in the course materials and drawing inspiration from established serverless benchmarks like SeBS \cite{sebs2021}, to execute experiments and collect all performance/cost data.
    \item \textbf{Phase 4 (Nov 17 - Nov 30):} Data analysis, graph generation, and creation of the final project report.
\end{itemize}

% --- Potential Risks and Mitigations ---
% \section{Potential Risks and Mitigations}
% \begin{itemize}[leftmargin=*, itemsep=2ex]
%     \item \textbf{Risk 1: Runtime Dependency Management}
%     \begin{itemize}
%         \item \textbf{Description:} The five image processing functions rely on third-party Python libraries (e.g., \texttt{Pillow}) that are not included in the standard AWS Lambda runtime. Failure to correctly package and deploy these dependencies will prevent the pipeline from executing.
%         \item \textbf{Mitigation:} We will use \textbf{AWS Lambda Layers} to manage these common dependencies. As part of our Phase 1 "Feasibility Spike," we will build and validate this layer with a manual test function \textit{before} proceeding with LLM generation. If this approach fails, we will use \textbf{Container Image} deployments as a backup solution.
%     \end{itemize}

%     \item \textbf{Risk 2: AWS Budget Overrun}
%     \begin{itemize}
%         \item \textbf{Description:} Executing 100+ runs for 6 pipeline versions, and calculating costs for 100,000 images, may lead to expenses that exceed our student or free-tier budgets.
%         \item \textbf{Mitigation:} We will first conduct small-scale tests (e.g., 100 invocations) to accurately estimate the cost-per-run. We will set strict \textbf{AWS Billing Alarms} and closely monitor our spending. All large-scale cost calculations (e.g., 100,000 images) will be extrapolated from this data rather than executed directly if budgets are a concern.
%     \end{itemize}

%     \item \textbf{Risk 3: Prompt Consistency and Quality}
%     \begin{itemize}
%         \item \textbf{Description:} Crafting prompts that are truly "identical" and produce functional, comparable code across three distinct LLMs is challenging. Different models may interpret instructions differently, leading to variations that
%     are not caused by the model's "optimization" but by a misunderstanding of the prompt.
%         \item \textbf{Mitigation:} We will design a single, detailed base prompt template. This template will be provided to all LLMs. Any necessary, minor, model-specific tuning (e.g., formatting instructions) will be documented. All final prompts used will be included as an appendix in our final report to ensure transparency and reproducibility.
%     \end{itemize}
% \end{itemize}

% --- References ---
% \newpage % References on a new page
% \section*{References}
\begin{thebibliography}{9}

\bibitem{firecracker}
Alexandru Agache, Marc Brooker, Andreea Florescu, Alexandra Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, and Diana-Maria Popa. 2020.
Firecracker: Lightweight Virtualization for Serverless Applications.
In \textit{17th USENIX Symposium on Networked Systems Design and Implementation (NSDI '20)}.
USENIX Association, 419–434.
\url{https://www.usenix.org/conference/nsdi20/presentation/agache}

\bibitem{chen2023}
Xinghan Chen, Ling-Hong Hung, Robert Cordingly, and Wes Lloyd. 2023.
X86 vs. ARM64: An Investigation of Factors Influencing Serverless Performance.
In \textit{24th ACM/IFIP International Middleware Conference (Middleware 2023), December 11-15, 2023, Bologna, Italy.}
ACM, New York, NY, USA, 6 pages.
\url{https://doi.org/10.1145/3631295.3631394}

\bibitem{sebs2021}
Marcin Copik, Grzegorz Kwaśniewski, Maciej Besta, Michał Podstawski, and Torsten Hoefler. 2021.
SeBS: A Serverless Benchmark Suite for Function-as-a-Service Computing.
In \textit{22nd International Middleware Conference (Middleware '21')}.
ACM, New York, NY, USA, 15 pages.
\url{https://doi.org/10.1145/3464298.3476133}

\end{thebibliography}

\end{document}
