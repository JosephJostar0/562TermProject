%% Based on ACM sigconf template, customized for TCSS 562 course requirements
%%
\documentclass[sigconf]{acmart}

%% ------------------------------------------------------------------
%% Preamble Settings
%% ------------------------------------------------------------------

%% For Prompt Display in Appendix
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % Added for beautiful tables
\usepackage{multirow} % Added for multirow cells in tables

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xurl}

\graphicspath{{./img/}}

\settopmatter{printacmref=false} 
\setcopyright{none}              
\renewcommand\footnotetextcopyrightpermission[1]{} % Failsafe to ensure footer is empty

\lstdefinelanguage{markdown}{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    columns=fullflexible,
    keepspaces=true,
    showstringspaces=false,
    backgroundcolor=\color{gray!5},
    moredelim=[is][\bfseries]{**}{**}, % Make bold text in markdown bold in latex
}

%% BibTeX logo command
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\acmConference[TCSS 562]{Cloud Computing Term Project}{December 2025}{Tacoma, WA}
% \acmISBN{978-1-4503-XXXX-X/25/12}
% \acmDOI{XXXXXXX.XXXXXXX}

%% ------------------------------------------------------------------
%% Document Body
%% ------------------------------------------------------------------
\begin{document}

%% ==================================================================
%% 1. Title Information
%% ==================================================================
\title[LLM-Generated Serverless Pipelines Case Study]{A Case Study on the Performance, Cost, and Architectural Implications of LLM-Generated Serverless Image Processing Pipelines}
\subtitle{TCSS 562 Cloud Computing Term Project}

%% ==================================================================
%% 2. Author Information
%% ==================================================================

%% Author 1
\author{Bohan Xiong}
\email{bohanx2@uw.edu}
\affiliation{%
  \institution{University of Washington Tacoma}
  \department{School of Engineering \& Technology}
  \city{Tacoma}
  \state{WA}
  \country{USA}
}

%% Author 2
\author{Xu Zhu}
\email{xuz44@uw.edu}
\affiliation{%
  \institution{University of Washington Tacoma}
  \department{School of Engineering \& Technology}
  \city{Tacoma}
  \state{WA}
  \country{USA}
}

%% Author 3
\author{Xiaoling Wei}
\email{xlwei28@uw.edu}
\affiliation{%
  \institution{University of Washington Tacoma}
  \department{School of Engineering \& Technology}
  \city{Tacoma}
  \state{WA}
  \country{USA}
}

%% Short Author List (For page headers)
\renewcommand{\shortauthors}{Xiong, Zhu, \& Wei}

%% ==================================================================
%% 3. Abstract
%% ==================================================================
\begin{abstract}
\sloppy
This project evaluates the performance and cost of serverless image processing pipelines generated by three LLMs: GPT-5.1 Thinking, Gemini 3.0 Pro, and DeepSeek V3.2. We deployed the pipelines on AWS Lambda using both x86\_64 and ARM64 architectures. Our results show that the ARM64 architecture consistently outperformed x86\_64, reducing costs by approximately 27\% and improving execution speed by 6.8\% to 11.8\% across all workloads. Regarding model comparisons, while DeepSeek V3.2 ran fastest on x86, it failed to generate correct code for library imports. In contrast, Gemini 3.0 Pro showed better compatibility with ARM, achieving a 38\% speedup in the compute-intensive color depth function. Additionally, a control group test using identical code revealed a 16\% performance difference, indicating significant environmental variance in the public cloud.
\end{abstract}

%% 4. CCS Concepts
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010521.10010537</concept_id>
  <concept_desc>Computer systems organization~Cloud computing</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178</concept_id>
  <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Cloud computing}
\ccsdesc[300]{Computing methodologies~Artificial intelligence}

%% 5. Keywords
\keywords{Serverless, FaaS, AWS Lambda, LLM, Image Processing, Microservices, ARM, x86}

%% Generate Title Page
\maketitle

%% ==================================================================
%% Main Content
%% ==================================================================

\section{Introduction}
\sloppy 
Serverless computing, specifically Function-as-a-Service (FaaS), has revolutionized cloud application development by abstracting infrastructure management and enabling granular scaling. Platforms like AWS Lambda utilize lightweight virtualization technologies such as Firecracker \cite{firecracker} to provide rapid provisioning. Simultaneously, Large Language Models (LLMs) are increasingly being adopted as "AI Assistants" for software development, yet their impact on the \textit{runtime efficiency} and \textit{architectural compatibility} of cloud-native applications remains underexplored.

In this project, we implement an enhanced Image Processing Pipeline as a modular serverless system, inspired by standard serverless benchmarks \cite{sebs2021}. We employed three state-of-the-art LLMs—GPT-5.1 Thinking, Gemini 3.0 Pro, and DeepSeek V3.2—to generate the implementation code for a five-stage pipeline using Python 3.14. The pipeline includes varied computational loads, ranging from CPU-intensive floating-point operations to I/O-bound S3 uploads, allowing for a multifaceted evaluation of generated code quality.

Our experiments uncovered significant divergences in how these models approach serverless development. While DeepSeek V3.2 demonstrated superior raw execution speed on x86\_64 architectures, it suffered from critical "API Hallucinations," failing to generate valid import statements for library-dependent tasks. Conversely, Gemini 3.0 Pro prioritized code robustness and exhibited a unique architectural affinity with ARM64 (Graviton2), achieving a 38\% performance speedup in compute-bound stages compared to x86. Furthermore, we validated the economic impact of these architectural decisions, observing a "Compound Effect" where the combination of ARM's lower unit price and reduced execution time yielded cost savings of over 27\%.

\subsection{Research Questions}
We investigate the following research questions regarding the performance, cost-efficiency, and architectural implications of our serverless implementation:

\begin{description}
    \item[RQ-1: Code Quality vs. Performance] How does the choice of Large Language Model (GPT-5.1, Gemini 3.0, DeepSeek 3.2) impact the trade-off between execution speed, code correctness (validity), and robustness (error handling)?
    \item[RQ-2: Architectural Impact] How does the underlying Function-as-a-Service CPU architecture (Intel x86\_64 vs. ARM Graviton2) impact the throughput and hosting costs of these LLM-generated pipelines? Specifically, does LLM-generated code exhibit "architectural bias" that favors one processor type over another in compute-bound versus I/O-bound tasks?
\end{description}

\section{Case Study}
This section details the serverless application design and the specific cloud services utilized. We implemented the pipeline as a set of decoupled, independent microservices on AWS Lambda.

\subsection{Serverless Application Implementation}
The application pipeline consists of five distinct logical steps, processed sequentially:

\begin{enumerate}
    \item \textbf{Function 1 (Greyscale):} Converts the input image to greyscale.
    \item \textbf{Function 2 (Resize):} Resizes the image to a fixed resolution (e.g., 800x600).
    \item \textbf{Function 3 (Color Depth Map):} Performs a CPU-intensive operation, mapping a 10-bit color depth image to 8-bit.
    \item \textbf{Function 4 (Rotate):} Rotates the resized image by 90 degrees.
    \item \textbf{Function 5 (Format Convert):} Performs an I/O-intensive operation, writing the final image as a different format (e.g., from JPEG to PNG) to the destination S3 bucket.
\end{enumerate}

\subsubsection{Dependency Management}
A key implementation challenge was that the default Python runtime in AWS Lambda does not contain the \texttt{Pillow} (PIL) library, which is essential for image processing. We resolved this by creating a custom Lambda Layer. To ensure cross-architecture compatibility (specifically building ARM64 layers on AMD64 development machines), we utilized Docker to build the layers in a target-specific environment.

\subsubsection{Prompt Engineering Strategy}
To ensure the LLMs generated robust, production-ready serverless code, we employed a systematic prompt engineering strategy. We treated the LLMs as "Expert Serverless Cloud Architects" and provided highly structured markdown specifications for each function. As detailed in Appendix \ref{sec:appendix_prompts}, each prompt included:
\begin{itemize}
    \item \textbf{Constraint Checklist:} Explicit technical constraints (e.g., "Use Pillow", "Handle all exceptions", "Return success: false on failure").
    \item \textbf{I/O Specifications:} Strict JSON schemas for input payloads and expected output formats to ensure interoperability between microservices.
    \item \textbf{Logic Requirements:} Step-by-step pseudo-code instructions, specifically enforcing where to start and stop performance timers to accurately measure execution logic separate from cold-starts.
\end{itemize}
This structured approach minimized hallucinations and ensured uniform error handling across code generated by different models.

\subsubsection{Code Generation}
To investigate RQ-1, we generated three distinct versions of this pipeline using identical prompts provided to GPT-5.1 Thinking, Gemini 3.0 Pro, and DeepSeek V3.2. The complete source code and experimental scripts are hosted on GitHub: \url{https://github.com/JosephJostar0/562TermProject}.

\subsection{Design Tradeoffs}
We analyzed several design decisions during the architectural phase:
\begin{itemize}
    \item \textbf{Service Composition:} We chose a decoupled microservice architecture over a monolithic function. This allows for granular scaling and precise performance measurement of each logical step.
    \item \textbf{Architecture Selection:} We investigate the tradeoff between Intel (x86\_64) and ARM (Graviton2) architectures. Table \ref{tab:pricing} details the pricing structure for AWS Lambda (us-east-2) \cite{awspricing}. ARM architecture offers a base price approximately 20\% lower than x86\_64. While this theoretical cost advantage is attractive, its actual impact on performance varies by workload and requires empirical validation \cite{chen2023}.
\end{itemize}

\begin{table}[h]
  \caption{AWS Lambda Pricing Structure (us-east-2)}
  \label{tab:pricing}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{llrr}
    \toprule
    Architecture & Tier (GB-seconds/month) & Price per GB-s & Request Price \\
    \midrule
    \textbf{x86\_64} & First 6 Billion & \$0.0000166667 & \$0.20/1M \\
                     & Next 9 Billion  & \$0.0000150000 & \$0.20/1M \\
                     & Over 15 Billion & \$0.0000133334 & \$0.20/1M \\
    \midrule
    \textbf{ARM64}   & First 7.5 Billion & \$0.0000133334 & \$0.20/1M \\
                     & Next 11.25 Billion& \$0.0000120001 & \$0.20/1M \\
                     & Over 18.75 Billion& \$0.0000106667 & \$0.20/1M \\
  \bottomrule
\end{tabular}
}
\end{table}

\subsection{Experimental Approach}
To evaluate the design tradeoffs, we deployed 6 pipeline versions (3 LLMs $\times$ 2 Architectures) comprising 30 AWS Lambda functions. We utilized a custom test harness to execute the experiments. For each function, we performed 5 warm-up runs followed by 50 benchmark runs. We used three input image resolutions (512$\times$512, 1080p, and 4K) to evaluate performance across varying workload intensities.

\section{Experimental Results}
Table \ref{tab:execution_time_comparison} summarizes the end-to-end pipeline execution time across three workload intensities.

\begin{table}[h]
  \caption{Average Pipeline Turnaround Time (ms) by Workload}
  \label{tab:execution_time_comparison}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{llrrr}
    \toprule
    & & \multicolumn{3}{c}{\textbf{Mean Execution Time (ms)}} \\
    \cmidrule(lr){3-5}
    \textbf{Architecture} & \textbf{LLM Source} & \textbf{Light} & \textbf{Standard} & \textbf{Heavy} \\
    \midrule
    \textbf{ARM64} & Gemini 3.0 Pro   & 753.68          & \textbf{2842.64} & \textbf{18156.91} \\
                   & DeepSeek V3.2    & \textbf{711.97} & 2843.13          & 18168.25 \\
                   & GPT-5.1 Thinking & 740.27          & 2862.89          & 18168.33 \\
    \midrule
    \textbf{x86\_64} & Gemini 3.0 Pro   & 835.93          & 3174.97          & 19923.82 \\
                     & DeepSeek V3.2    & \textbf{806.92} & \textbf{3101.02} & \textbf{19496.17} \\
                     & GPT-5.1 Thinking & 827.98          & 3173.27          & 19584.24 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

The updated results highlight a distinct architectural preference for each model. \textbf{DeepSeek V3.2} demonstrates superior performance on the x86\_64 architecture across all workload intensities and leads the ARM64 Light workload. Conversely, \textbf{Gemini 3.0 Pro} performs best on ARM64 for Standard and Heavy workloads, albeit with a marginal advantage over DeepSeek (approx. 0.5ms to 12ms). \textbf{GPT-5.1 Thinking} generally maintains a middle ground, showing consistent stability but rarely achieving the lowest latency. Overall, the ARM64 architecture provided faster execution times than x86\_64 for all models.

\subsection{RQ-1: Impact of LLM Choice}
We evaluated the performance differences across the three LLMs. Our analysis reveals that model choice impacts not just performance, but the fundamental validity and robustness of the generated code.

\subsubsection{Code Validity \& Hallucinations (Function 2)}
A critical finding in Function 2 (Resize) was DeepSeek V3.2's failure to generate functional code. The model hallucinated a non-existent import, \texttt{from PIL import ImageResampling}, treating a class attribute as a top-level module. This "API Hallucination" rendered the code unexecutable. To proceed with the experiment, we were forced to substitute GPT-5.1's code for the DeepSeek pipeline in this specific function. This result highlights a significant gap in "correctness" for DeepSeek compared to the other models, despite its high performance in other steps.

\subsubsection{Defensive Overhead vs. Minimalist Efficiency (Function 1)}
In I/O-bound tasks like Function 1 (Greyscale), we observed two distinct behavioral archetypes. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{primary_llm_Step_1_Greyscale.png}
  
  \caption{Step 1 (Greyscale) Performance. GPT-5.1's defensive logic (checking for alpha channels) resulted in significantly higher latency compared to the streamlined implementations of Gemini and DeepSeek.}
  \label{fig:step1}
\end{figure}

GPT-5.1 prioritized robustness by implementing automatic Alpha channel detection and conditional PNG encoding. While ensuring correctness for transparent images, this logic increased both processing latency and payload size. In contrast, Gemini and DeepSeek adhered to a minimalist implementation. Structurally, DeepSeek optimized efficiency through chained method calls, whereas Gemini used verbose sequential assignments, which unnecessarily extended object lifecycles and increased memory pressure.

\subsubsection{Library Usage \& Type Handling (Function 3)}
Function 3 (Color Depth Map) demonstrated the most significant architectural performance gap in the pipeline (Figure \ref{fig:step3}). On ARM64, Gemini 3.0 Pro achieved a dominant performance advantage, executing the Heavy workload in $\approx$290ms compared to $>$410ms for DeepSeek and GPT. On x86\_64, however, DeepSeek and Gemini performed similarly ($\approx$460ms), while GPT-5.1 lagged behind.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{primary_llm_Step_3_ColorDepth.png}
  \caption{Step 3 (Color Depth) Performance. Gemini 3.0 Pro exhibits exceptional efficiency on ARM, significantly outperforming competitors. On x86, Gemini and DeepSeek are comparable, while GPT demonstrates the highest latency.}
  \label{fig:step3}
\end{figure}

The drastic reduction in latency for Gemini on ARM suggests its specific implementation details (e.g., explicit integer casting versus floating-point operations) align more effectively with the ARM processor's optimization capabilities than the code generated by other models.

\subsubsection{The "Robustness Tax" vs. Minimalist Speed (Function 4)}
Function 4 (Rotate) highlights a distinct trade-off between raw execution speed and code safety. As illustrated in Figure \ref{fig:step4}, while performance was comparable across all models on the x86 architecture, a noticeable gap emerged on ARM.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{primary_llm_Step_4_Rotate.png}
  \caption{Step 4 (Rotate) Performance. On the ARM architecture, Gemini incurred a "robustness tax" due to safety checks, whereas DeepSeek and GPT achieved faster execution times by omitting these validations.}
  \label{fig:step4}
\end{figure}

Code analysis reveals that this gap stems from a "Robustness Tax." Gemini's implementation includes explicit mode handling (\texttt{if img.mode != 'RGB'}) to safely remove transparency. In contrast, DeepSeek and GPT-5.1 omit this logic. While their "minimalist" approach is faster on ARM, it causes an \texttt{OSError} when processing transparent images (e.g., RGBA PNGs), as the JPEG format does not support alpha channels. Gemini prioritizes production stability over raw speed.

\subsubsection{I/O Dominance \& Performance Uniformity (Function 5)}
Function 5 (Format Convert \& Upload) demonstrates the characteristics of an I/O-bound task. As illustrated in Figure \ref{fig:step5}, all three LLMs exhibited nearly identical performance profiles across all workloads and architectures.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{primary_llm_Step_5_Upload.png}
  \caption{Step 5 (Upload) Performance. The execution time is dominated by network I/O (uploading to S3), resulting in uniform performance across all LLMs. Computational differences are masked by network latency.}
  \label{fig:step5}
\end{figure}

The data indicates that the bottleneck for this function is the network bandwidth required to upload the processed images to the S3 bucket, rather than code efficiency. In the "Heavy" workload (4K images), the execution time surged to approximately 16–18 seconds for all models. This confirms that for I/O-intensive serverless functions, the choice of LLM for code generation has a negligible impact on performance compared to external infrastructure limits.

\subsection{RQ-2: Architectural Implications (x86 vs ARM)}
Our secondary investigation focuses on the impact of migrating from x86\_64 to ARM64 (Graviton2). We analyzed the speedup and performance characteristics across different workload intensities.

\subsubsection{ARM Speedup Overview}
Figure \ref{fig:arch_speedup} illustrates the relative performance gain achieved by the ARM migration. The results confirm a universal performance improvement across all LLM-generated pipelines.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{secondary_speedup_Pipeline_Total.png}
  \caption{Pipeline Total Speedup (ARM vs. x86). The ARM architecture delivered consistent performance improvements across all models and workloads, ranging from 6.8\% to 11.8\%.}
  \label{fig:arch_speedup}
\end{figure}

Regardless of the model used, the ARM architecture consistently outperformed x86. In the heavy workload configuration, speedups ranged from approximately 7\% to 9\%. Lighter workloads exhibited slightly higher relative gains (up to 11.8\%). This data demonstrates that the performance advantage of the Graviton2 processor is robust and independent of the specific code implementation differences among the LLMs.

\subsubsection{Cost Analysis: The Compound Effect}
Migrating to ARM reduced costs through two multiplicative factors: the lower hourly rate of Graviton2 instances and the reduced execution time. We quantify the total cost saving percentage ($S$) using Equation \ref{eq:cost_saving}:

\begin{equation}
  S = \left( 1 - \frac{T_{ARM} \cdot P_{ARM}}{T_{x86} \cdot P_{x86}} \right) \times 100\%
  \label{eq:cost_saving}
\end{equation}

where $T$ represents the total execution time and $P$ represents the unit price per GB-second. Given that AWS prices ARM Lambda instances approximately 20\% lower than x86 ($P_{ARM} \approx 0.8 P_{x86}$), any performance speedup ($T_{ARM} < T_{x86}$) further amplifies the total savings beyond the baseline discount.

Table \ref{tab:cost_comparison} presents the empirical results for the heavy workload.

\begin{table}[h]
\centering
\caption{Performance \& Realized Cost Savings (Heavy Workload)}
\label{tab:cost_comparison}
\setlength{\tabcolsep}{3.5pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Runtime (ms)}} & \\
\cmidrule(lr){2-3}
\textbf{LLM Source} & \textbf{x86} & \textbf{ARM} & \textbf{\shortstack{Realized\\Savings ($S$)}} \\
\midrule
DeepSeek V3.2    & 19,496 & 18,168 & 25.45\% \\
GPT-5.1 Thinking & 19,584 & 18,168 & 25.78\% \\
Gemini 3.0 Pro   & 19,923 & 18,156 & 27.09\% \\
\bottomrule
\end{tabular}
}
\end{table}

Figure \ref{fig:cost_savings} visualizes this "Compound Effect." The realized savings consistently exceeded the 20\% pricing baseline (represented by the blue dashed line). For the heavy workload, Gemini 3.0 Pro achieved the highest efficiency (27.1\%) due to its superior runtime reduction. However, in light workloads, DeepSeek V3.2 demonstrated the peak cost efficiency of the entire study, reaching 29.4\% savings. This confirms that ARM migration offers strictly dominant economic benefits, with returns increasing as the execution speedup improves.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{secondary_cost_savings.png}
  \caption{Real Cost Savings (ARM vs. x86). All scenarios exceeded the 20\% baseline. Savings peaked at 29.4\% for DeepSeek in the light workload, driven by the substantial relative speedup in that configuration.}
  \label{fig:cost_savings}
\end{figure}

\subsubsection{Instruction Set Affinity (Function 3 Case Study)}
The performance differential in Function 3 (Color Depth Map) offers the clearest evidence of how implementation details interact with architectural strengths. While the ARM64 architecture provided a baseline speedup across most functions, Gemini 3.0 Pro's implementation saw a disproportionate benefit in this specific step.

Comparing the heavy workload execution times, Gemini achieved a 38\% latency reduction on ARM64 (from 468ms on x86 to 290ms), whereas DeepSeek and GPT-5.1 only saw reductions of approximately 9\% to 14\%.

Code analysis indicates that this disparity stems from data type handling. Gemini utilized explicit integer casting (\texttt{int()}) for pixel value mapping, while the other models relied on implicit floating-point operations. The Graviton2 processor processes these integer-based arithmetic instructions significantly more efficiently than the equivalent floating-point logic. This suggests that maximizing the return on investment from ARM migration requires "type-aware" code optimizations that align with the underlying instruction set.

\subsubsection{Environmental Variance (Control Group Analysis)}
\sloppy
Function 2 (Resize) served as a control group where, due to the generation failure noted in RQ1, the DeepSeek pipeline utilized the exact same code as the GPT-5.1 pipeline. Theoretically, the architectural speedup should be identical.

However, empirical data revealed significant inconsistency: the DeepSeek group recorded a \textbf{28.7\%} speedup on ARM (dropping from 659ms to 470ms), while the GPT group saw only a \textbf{12.6\%} gain (643ms to 562ms). Since the software artifact was identical, this 16-point divergence isolates the "environmental noise" of the public cloud. It demonstrates that serverless performance is heavily influenced by the specific hardware instance assignment, suggesting that performance margins narrower than $\approx$15\% should be interpreted with caution.

\subsubsection{Limits of Acceleration (I/O Bottleneck)}
Function 5 (Upload) illustrates Amdahl's Law, where network latency ($\approx$17s) dominated the heavy workload. While ARM retained a slight 5--8\% advantage (likely due to efficient TLS processing), this massive fixed I/O cost heavily diluted the substantial computational gains achieved in previous steps. Consequently, the relative architectural speedup diminishes significantly when the pipeline becomes I/O-bound.

\section{Future Work}
Based on our current findings, we propose several avenues for extending this research to further optimize serverless image processing pipelines.

\subsection{No-GIL Python 3.14 Evaluation}
With the release of Python 3.14 \cite{python314}, specifically the experimental free-threading build (No-GIL) defined in PEP 703 \cite{pep703}, there is significant potential to improve parallelism within individual Lambda function invocations. Although the default distribution still retains the Global Interpreter Lock (GIL), we plan to compile and test the free-threaded version to evaluate its impact on concurrent image processing tasks within a single high-memory Lambda instance.

\subsection{LLM-Generated Cython Optimization}
To push the performance boundaries further, we aim to investigate the capability of LLMs to generate Cython code \cite{cython}. By compiling performance-critical sections of the pipeline into C extensions, we expect to see substantial reductions in execution time, particularly for pixel-manipulation tasks like the Color Depth Map stage.

\section{Conclusions}
In this study, we successfully implemented and benchmarked a modular serverless image processing pipeline on AWS Lambda using Python 3.14. Our comparative analysis of LLM-generated code reveals a critical trade-off between raw execution speed, code validity, and architectural affinity.

\textbf{Model Capabilities and Trade-offs:}
\begin{itemize}
    \item \textbf{DeepSeek V3.2} exhibited a "high-risk, high-reward" behavior. While it demonstrated superior raw performance on the x86\_64 architecture and minimalist efficiency, it suffered from significant correctness issues, including API hallucinations (Function 2) and a lack of safety checks for edge cases (Function 4).
    \item \textbf{GPT-5.1 Thinking} adopted a highly defensive coding style. While this ensured stability, the associated overhead (e.g., redundant channel detection) consistently placed it behind competitors in terms of latency.
    \item \textbf{Gemini 3.0 Pro} emerged as the most production-ready option. It not only prioritized system stability (paying a "robustness tax" to handle transparent images safely) but also demonstrated exceptional architectural affinity with ARM64. Its use of integer-based logic in pixel manipulation allowed it to outperform competitors by significant margins on Graviton2 processors.
\end{itemize}

\textbf{Architectural and Economic Implications:}
Our results confirm that migrating to the ARM64 (Graviton2) architecture yields strictly dominant benefits. The migration delivered a universal performance speedup (6.8\% to 11.8\%) regardless of the code source. Furthermore, due to the "Compound Effect" of reduced runtime and lower unit pricing, we observed realized cost savings between 25.4\% and 29.4\%, significantly exceeding the baseline pricing difference.

\textbf{Limitations:}
We identified an "I/O Wall" where network latency dominates compute-heavy tasks, effectively normalizing performance across all models. Additionally, our control group analysis revealed that public cloud environmental noise can cause performance variances of up to 16\%, suggesting that minor speedups should be interpreted with caution.

Future serverless implementations should prioritize "type-aware" code generation to fully leverage hardware capabilities and carefully validate LLM-generated logic for hallucinated dependencies before deployment.

\begin{acks}
We would like to thank the teaching assistants and the professor of TCSS 562 for their guidance. Specifically, the course tutorials provided invaluable hands-on assistance, including "Tutorial 4 - Intro to FaaS - AWS Lambda", "Tutorial 5 - Intro to FaaS II - Working with Files in S3, Cloud Trail, and Amazon Event Bridge rules", "Tutorial 6 - Intro to FaaS III - Serverless Databases", and "Tutorial 7 - Intro to Docker Containerization".
\end{acks}

%% ==================================================================
%% References
%% ==================================================================
\begin{thebibliography}{9}

\bibitem{firecracker}
Alexandru Agache, Marc Brooker, Andreea Florescu, Alexandra Iordache, Anthony Liguori, Rolf Neugebauer, Phil Piwonka, and Diana-Maria Popa.
2020. Firecracker: Lightweight Virtualization for Serverless Applications. In \textit{17th USENIX Symposium on Networked Systems Design and Implementation (NSDI '20)}.
USENIX Association, 419--434. \url{https://www.usenix.org/conference/nsdi20/presentation/agache}

\bibitem{sebs2021}
Marcin Copik, Grzegorz Kwaśniewski, Maciej Besta, Michał Podstawski, and Torsten Hoefler.
2021. SeBS: A Serverless Benchmark Suite for Function-as-a-Service Computing. In \textit{22nd International Middleware Conference (Middleware '21')}.
ACM, New York, NY, USA, 15 pages. \url{https://doi.org/10.1145/3464298.3476133}

\bibitem{awspricing}
Amazon Web Services, Inc. 2025. AWS Lambda Pricing.
Retrieved from \url{https://aws.amazon.com/lambda/pricing/}

\bibitem{chen2023}
Xinghan Chen, Ling-Hong Hung, Robert Cordingly, and Wes Lloyd.
2023. X86 vs. ARM64: An Investigation of Factors Influencing Serverless Performance. In \textit{24th ACM/IFIP International Middleware Conference (Middleware 2023)}.
ACM, New York, NY, USA, 6 pages. \url{https://doi.org/10.1145/3631295.3631394}

\bibitem{python314}
Python Software Foundation.
2024. Python 3.14.0a1 Release. Retrieved from \url{https://www.python.org/downloads/release/python-3141/}

\bibitem{pep703}
Sam Gross. 2023. PEP 703 – Making the Global Interpreter Lock Optional in CPython.
Retrieved from \url{https://peps.python.org/pep-0703/}

\bibitem{cython}
Cython Developers. 2024. Cython: C-Extensions for Python. Retrieved from \url{https://cython.org/}

\end{thebibliography}

%% ==================================================================
%% Appendix
%% ==================================================================
\appendix

\section{Prompt Engineering Specifications}
\label{sec:appendix_prompts}

The following sections detail the exact prompts provided to the Large Language Models to generate the serverless functions. These prompts enforce the "Expert Serverless Cloud Architect" persona and strict technical constraints.

\subsection{Function 1: Greyscale}
\begin{lstlisting}[language=markdown]
# Context
You are an expert Serverless Cloud Architect and Python Developer. Your task is to write a highly optimized AWS Lambda function handler in **Python 3.14**.

# Constraint Checklist & Confidence Score
1. Use the `Pillow` (PIL) library for image processing. Assume it is already installed in the AWS Lambda Layer (do NOT use `pip install`).
2. Use standard libraries: `json`, `base64`, `io`, `time`.
3. **Robustness:** The function must NEVER crash. It must handle all exceptions (e.g., bad Base64, invalid image data) and return a JSON response with `"success": false` and the error message.
4. **Performance:** The code must run efficiently on both x86_64 and arm64 architectures.

# Function Specifications (Function 1: Greyscale)

## Goal
Convert an input Base64-encoded image string into a Greyscale image and return it as a Base64 string.

## Input Payload (JSON)
The Lambda `event` object will contain a body (which might be a JSON string or a dict, handle both). Structure:
```json
{
  "image": "base64_encoded_string_of_input_image...",
  "params": {}
}
```

## Output Payload (JSON)

You must return a JSON object (dictionary) strictly adhering to this schema:

```json
{
  "success": boolean,
  "image": "base64_encoded_string_of_output_image_or_null_if_failed",
  "execution_time_ms": float,
  "error": "error_message_string_or_null"
}
```

## Specific Logic Requirements

1.  **Timing (`execution_time_ms`):**
      - Start the timer **immediately before** you attempt to decode the input Base64 string.
      - Stop the timer **immediately after** you define the final output Base64 string.
      - This ensures we measure the cost of I/O serialization/deserialization + Image Processing.
2.  **Image Processing:**
      - Decode the base64 string to bytes.
      - Open the image using `PIL.Image`.
      - Convert the image to Greyscale (mode 'L').
      - Save the resulting image to a memory buffer (`io.BytesIO`).
      - **Critical:** To prevent payload size explosion, save the output image as **JPEG** with `quality=85` (unless the input suggests a specific need for PNG, but default to JPEG for efficiency).
      - Encode the buffer back to a Base64 string.

# Task

Generate the complete `lambda_handler(event, context)` Python code. Do not include explanations, just the code.
\end{lstlisting}

\subsection{Function 2: Resize}
\begin{lstlisting}[language=markdown]
# Context
You are an expert Serverless Cloud Architect and Python Developer. Your task is to write a highly optimized AWS Lambda function handler in **Python 3.14**.

# Constraint Checklist & Confidence Score
1. Use the `Pillow` (PIL) library for image processing. Assume it is already installed in the AWS Lambda Layer.
2. **Compatibility:** Use Modern Pillow syntax (e.g., `Image.Resampling.LANCZOS` instead of `Image.ANTIALIAS`).
3. **Robustness:** The function must NEVER crash. Handle exceptions and return `"success": false` with error details.
4. **Performance:** Optimize for fast execution on both x86_64 and arm64.

# Function Specifications (Function 2: Resize)

## Goal
Resize the input image to a specific resolution (Target: 800x600) to reduce payload size for downstream functions.

## Input Payload (JSON)
Structure:
```json
{
  "image": "base64_encoded_string...",
  "params": { 
      "width": 800, 
      "height": 600 
  }
}
```

*Note: If `params` are missing, default to width=800, height=600.*

## Output Payload (JSON)

Structure:

```json
{
  "success": boolean,
  "image": "base64_encoded_string_resized...",
  "execution_time_ms": float,
  "error": "string_or_null"
}
```

## Specific Logic Requirements

1.  **Timing (`execution_time_ms`):**
      - Measure the **entire** process: Base64 Decode -> Resize Logic -> Base64 Encode.
2.  **Image Processing:**
      - Decode the base64 string.
      - Parse `width` and `height` from `params` (default to 800x600 if integer values are missing).
      - **Operation:** Resize the image to the exact target dimensions `(width, height)`.
      - **Resampling Method:** Use `Image.Resampling.LANCZOS` (High quality downsampling).
      - **Output Format:** Save to buffer as **JPEG** (Standardize format), `quality=85`.
      - Encode back to Base64.

# Task

Generate the complete `lambda_handler(event, context)` Python code. Do not include explanations, just the code.
\end{lstlisting}

\subsection{Function 3: Color Depth Map}
\begin{lstlisting}[language=markdown]
# Context

You are an expert Serverless Cloud Architect and Python Developer. Your task is to write a highly optimized AWS Lambda function handler in **Python 3.14**.

# Constraint Checklist & Confidence Score

1. Use the `Pillow` (PIL) library.
2. **CPU Intensity:** The logic MUST perform mathematical calculations to simulate high-quality image processing (e.g., Tone Mapping / Gamma Correction) on **high bit-depth data**. All math must be floating-point and executed per-pixel.
3. **Robustness:** Catch all exceptions. Return `"success": false` on failure.
4. **Performance:** The goal is to benchmark CPU processing power (Floating Point Operations).
5. **CRITICAL FIX:** Every `.point(lambda p: ...)` lambda **MUST first check** whether `p` is an `int` or `float`. Pillow will call the lambda once with an `ImagePointTransform` object to test linearity, and this must not crash.

# Function Specifications (Function 3: Color Depth Map)

## Goal

Perform a CPU-intensive "Color Depth Reduction" simulation. To ensure high CPU usage and robustness against Pillow’s `.point()` evaluation behavior, the function must:

* Force-convert the image to a high-precision mode,
* Simulate 10-bit data,
* Apply heavy floating-point gamma correction,
* **Ensure all lambda functions in `.point()` are type-safe**.

## Input Payload (JSON)

```json
{
  "image": "base64_encoded_string...",
  "params": { "target_depth": 8 }
}
```

*If `target_depth` is missing, default to 8.*

## Output Payload (JSON)

```json
{
  "success": boolean,
  "image": "base64_encoded_string_processed...",
  "execution_time_ms": float,
  "error": "string_or_null"
}
```

## Specific Logic Requirements

### 1. Timing

Measure: **Base64 Decode → Math-Heavy Processing → Base64 Encode**
Return total duration as `execution_time_ms`.

---

### 2. Processing Pipeline (Strict Order)

#### **Step 1 — Base64 Decode**

Decode Base64 string → bytes → `PIL.Image`.

#### **Step 2 — Force Upscaling**

Convert image to **Mode `"I"`** (32-bit signed integer pixels).
This is required to allow values >255 and avoid LUT overflow.

#### **Step 3 — Simulate 10-bit Sensor Data**

Multiply each pixel by `4` (`0–255 → 0–1020`).
Use:

```
img.point(lambda p: p * 4 if isinstance(p, (int, float)) else 0)
```

The `isinstance` check is *mandatory* to avoid Pillow calling the lambda with `ImagePointTransform` and causing TypeError.

#### **Step 4 — CPU-Heavy Gamma Correction (Gamma = 2.2)**

Formula:

```
NewPixel = 255 * ((OldPixel / 1023.0) ** (1 / 2.2))
```

Implement using `.point(lambda p: ...)` with floating-point math.
The lambda must **also** check input type:

```
lambda p: 255.0 * ((p / 1023.0) ** inv_gamma)
          if isinstance(p, (int, float)) else 0
```

This forces per-pixel floating-point operations and avoids the Pillow evaluation crash.

#### **Step 5 — Downsampling**

Convert processed image to **mode `"L"`** (8-bit grayscale) or `"RGB"`.
Use `"L"` for deterministic JPEG output.

#### **Step 6 — Save to Buffer**

Save as **JPEG**, `quality=85`.

#### **Step 7 — Base64 Encode**

Encode JPEG bytes → Base64 string.

---

## Error Handling

* Wrap entire processing logic in `try/except`.
* On failure, return:

  ```json
  {
    "success": false,
    "image": "",
    "execution_time_ms": 0.0,
    "error": "error_message + traceback"
  }
  ```

---

## Task

Generate the complete `lambda_handler(event, context)` Python function code.
Do **not** include explanations — output **only** the code.
\end{lstlisting}

\subsection{Function 4: Rotate}
\begin{lstlisting}[language=markdown]
# Context
You are an expert Serverless Cloud Architect and Python Developer. Your task is to write a highly optimized AWS Lambda function handler in **Python 3.14**.

# Constraint Checklist & Confidence Score
1. Use the `Pillow` (PIL) library.
2. **Robustness:** Handle all exceptions. Return `"success": false` on failure.
3. **Consistency:** Ensure the rotation does not crop the image logic (handle canvas size changes).
4. **Performance:** Optimize for execution on x86_64 and arm64.

# Function Specifications (Function 4: Rotate)

## Goal
Rotate the input image by a specified angle (N degrees).

## Input Payload (JSON)
```json
{
  "image": "base64_encoded_string...",
  "params": { "angle": 90 }
}
```

*Note: If `angle` is not provided, default to 90 degrees.*

## Output Payload (JSON)

```json
{
  "success": boolean,
  "image": "base64_encoded_string_rotated...",
  "execution_time_ms": float,
  "error": "string_or_null"
}
```

## Specific Logic Requirements

1.  **Timing (`execution_time_ms`):**
      - Measure: Base64 Decode -> **Rotation Logic** -> Base64 Encode.
2.  **Image Processing:**
      - Decode Base64 to image.
      - Parse `angle` from `params` (default = 90).
      - **Operation:** Rotate the image.
      - **Crucial:** Use `.rotate(angle, expand=True)` to ensure the image frame resizes to accommodate the new orientation (preventing cropping).
      - Save to buffer as **JPEG**, `quality=85`.
      - Encode back to Base64.

# Task

Generate the complete `lambda_handler(event, context)` Python code. Do not include explanations, just the code.
\end{lstlisting}

\subsection{Function 5: Format Convert \& Upload}
\begin{lstlisting}[language=markdown]
# Context
You are an expert Serverless Cloud Architect and Python Developer. Your task is to write a highly optimized AWS Lambda function handler in **Python 3.14**.

# Constraint Checklist & Confidence Score
1. Use `boto3` for S3 interactions (standard AWS SDK).
2. Use `Pillow` (PIL) for image format conversion.
3. **I/O Intensity:** This function benchmarks network I/O.
4. **Robustness:** Handle S3 errors (e.g., `ClientError`) and Image errors gracefully.
5. **Global State:** Initialize the S3 client *outside* the handler to leverage execution context reuse.

# Function Specifications (Function 5: Format Convert & Upload)

## Goal
Convert the image format (e.g., JPEG to PNG) to increase file size/processing time, then upload the result to an AWS S3 Bucket.

## Input Payload (JSON)
```json
{
  "image": "base64_encoded_string...",
  "params": {
    "target_format": "PNG",
    "bucket_name": "your-target-bucket",
    "s3_key": "output/final_image.png"
  }
}
```

*Note: Use defaults if params are missing: target\_format="PNG", bucket\_name="test-bucket", s3\_key="output/test.png".*

## Output Payload (JSON)

```json
{
  "success": boolean,
  "s3_url": "[https://s3.region.amazonaws.com/bucket/key](https://s3.region.amazonaws.com/bucket/key)",
  "execution_time_ms": float,
  "error": "string_or_null"
}
```

## Specific Logic Requirements

1.  **Timing (`execution_time_ms`):**
      - Measure: Base64 Decode -> Format Conversion -> **S3 Upload (`put_object`)** -> End.
      - Do NOT include the S3 Client initialization time (do that globally).
2.  **Image Processing:**
      - Decode Base64 to image.
      - Convert the image to the `target_format` (Default: **PNG**).
      - Save to a memory buffer (`io.BytesIO`).
3.  **S3 Interaction (I/O):**
      - Upload the buffer contents to S3 using `s3_client.put_object()`.
      - Ensure you set the correct `ContentType` (e.g., `image/png`).
      - Construct the standard S3 URL for the response.

# Task

Generate the complete `lambda_handler(event, context)` Python code. Do not include explanations, just the code.
\end{lstlisting}

\end{document}
